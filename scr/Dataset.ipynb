{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# word_dict = {0:'aa', 1:'aw', 2:'dd', 3:'ee', 4:'oo', 5:'ow', 6:'uw'}\n",
    "word_dict = {0:'a', 1:'aa', 2:'aw', 3:'b', 4:'c', 5:'d', 6:'dd', 7:'e', 8:'ee', 9:'g', 10:'h', 11:'i', 12:'k', 13:'l', 14:'m', 15:'n', 16:'o', 17:'oo', 18:'ow', 19:'p', 20:'q', 21:'r', 22:'s', 23:'t', 24:'u', 25:'uw', 26:'v', 27:'x', 28:'y'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"C:/Code/HWR/MyCode/Resourses/dataset0/aa/70.png\"\n",
    "# directory = \"C:/Code/HWR/MyCode/Resourses/dataset0/aa\"\n",
    "# img = cv2.imread(path)\n",
    "# data = np.expand_dims(img, 0)\n",
    "\n",
    "# myImageGen = ImageDataGenerator(shear_range=40)\n",
    "# # Batch_Size= 1 -> Moi lan sinh ra 1 anh\n",
    "# gen = myImageGen.flow(data, batch_size=1)\n",
    "# os.chdir(directory)\n",
    "# count = 85\n",
    "# for i in range(9):\n",
    "# \tplt.subplot(330 + 1 + i)\n",
    "# \tmyBatch = gen.next()\n",
    "# \timage = myBatch[0].astype('uint8')\n",
    "# \tplt.imshow(image)\t\n",
    "# \tcv2.imwrite(f\"{count}.png\", image)\n",
    "# \tcount = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 28\n",
    "img_width = 28\n",
    "batch_size = 2\n",
    "# model = keras.Sequential([\n",
    "#     layers.Input((28, 28, 1)),\n",
    "#     layers.Conv2D(16, 3, padding='same'),\n",
    "#     layers.Conv2D(32, 3, padding='same'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(7),\n",
    "# ])\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape = (28,28,1), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(7,activation='softmax'))\n",
    "model.add(Dense(29,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5465 files belonging to 29 classes.\n",
      "Using 4919 files for training.\n",
      "Found 5465 files belonging to 29 classes.\n",
      "Using 546 files for validation.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2460/2460 [==============================] - 7s 3ms/step - loss: 2.2797 - accuracy: 0.4745\n",
      "Epoch 2/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.9450 - accuracy: 0.7359\n",
      "Epoch 3/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.6059 - accuracy: 0.8280\n",
      "Epoch 4/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.4162 - accuracy: 0.8742\n",
      "Epoch 5/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.3239 - accuracy: 0.9012\n",
      "Epoch 6/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.2557 - accuracy: 0.9246\n",
      "Epoch 7/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.2337 - accuracy: 0.9272\n",
      "Epoch 8/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1877 - accuracy: 0.9437\n",
      "Epoch 9/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1645 - accuracy: 0.9528\n",
      "Epoch 10/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1690 - accuracy: 0.9577\n",
      "Epoch 11/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1498 - accuracy: 0.9583\n",
      "Epoch 12/50\n",
      "2460/2460 [==============================] - 6s 3ms/step - loss: 0.1383 - accuracy: 0.9646\n",
      "Epoch 13/50\n",
      "2460/2460 [==============================] - 6s 3ms/step - loss: 0.1366 - accuracy: 0.9701\n",
      "Epoch 14/50\n",
      "2460/2460 [==============================] - 6s 3ms/step - loss: 0.1645 - accuracy: 0.9679\n",
      "Epoch 15/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1658 - accuracy: 0.9652\n",
      "Epoch 16/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.0985 - accuracy: 0.9764\n",
      "Epoch 17/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1418 - accuracy: 0.9734\n",
      "Epoch 18/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1381 - accuracy: 0.9738\n",
      "Epoch 19/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1567 - accuracy: 0.9730\n",
      "Epoch 20/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1170 - accuracy: 0.9782\n",
      "Epoch 21/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1281 - accuracy: 0.9807\n",
      "Epoch 22/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1207 - accuracy: 0.9803\n",
      "Epoch 23/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1270 - accuracy: 0.9813\n",
      "Epoch 24/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1089 - accuracy: 0.9841\n",
      "Epoch 25/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1530 - accuracy: 0.9799\n",
      "Epoch 26/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1404 - accuracy: 0.9801\n",
      "Epoch 27/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1115 - accuracy: 0.9817\n",
      "Epoch 28/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1631 - accuracy: 0.9823\n",
      "Epoch 29/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1839 - accuracy: 0.9797\n",
      "Epoch 30/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1264 - accuracy: 0.9860\n",
      "Epoch 31/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1370 - accuracy: 0.9823\n",
      "Epoch 32/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1031 - accuracy: 0.9876\n",
      "Epoch 33/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1899 - accuracy: 0.9845\n",
      "Epoch 34/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1676 - accuracy: 0.9845\n",
      "Epoch 35/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1042 - accuracy: 0.9882\n",
      "Epoch 36/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1540 - accuracy: 0.9839\n",
      "Epoch 37/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1233 - accuracy: 0.9878\n",
      "Epoch 38/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1333 - accuracy: 0.9888\n",
      "Epoch 39/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1160 - accuracy: 0.9896\n",
      "Epoch 40/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1015 - accuracy: 0.9890\n",
      "Epoch 41/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1430 - accuracy: 0.9856\n",
      "Epoch 42/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.0869 - accuracy: 0.9923\n",
      "Epoch 43/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1657 - accuracy: 0.9868\n",
      "Epoch 44/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1361 - accuracy: 0.9906\n",
      "Epoch 45/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1894 - accuracy: 0.9886\n",
      "Epoch 46/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1276 - accuracy: 0.9917\n",
      "Epoch 47/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1563 - accuracy: 0.9906\n",
      "Epoch 48/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1795 - accuracy: 0.9890\n",
      "Epoch 49/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1013 - accuracy: 0.9939\n",
      "Epoch 50/50\n",
      "2460/2460 [==============================] - 6s 2ms/step - loss: 0.1499 - accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../Resourses/Alphabet - data/',\n",
    "    # '../Resourses/dataset/',\n",
    "    labels = 'inferred',\n",
    "    label_mode=\"int\",\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    ")\n",
    "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../Resourses/Alphabet - data/',\n",
    "    # '../Resourses/dataset/',\n",
    "    labels = 'inferred',\n",
    "    label_mode=\"int\",\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "def augment(x, y):\n",
    "    image = tf.image.random_brightness(x, max_delta=0.05)\n",
    "    return image, y\n",
    "\n",
    "ds_train = ds_train.map(augment)\n",
    "\n",
    "# for epochs in range(10):\n",
    "#     for x, y in ds_train:\n",
    "#         pass\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss=[\n",
    "        keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    ],\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(ds_train, batch_size=batch_size, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 29)                45501     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,605\n",
      "Trainable params: 64,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# lưu mô hình traning\n",
    "model.save(r'model_hand.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(r'model_hand.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mCharacter Recognition\u001b[39m\u001b[39m'\u001b[39m, img)\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     k \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m27\u001b[39m:\n\u001b[0;32m     26\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Predection on External Image\n",
    "\n",
    "img = cv2.imread(r'C:/Code/HWR/MyCode/Resourses/image/aw.png')\n",
    "img_copy = img.copy()\n",
    "\n",
    "# chuyển hình đầu vào về kích cỡ 400 x 440 và sang màu xám -> để hình chỉ có 1 chiều (ảnh màu đầu vào sẽ có 3 chiều)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (400,440))\n",
    "\n",
    "\n",
    "img_copy = cv2.GaussianBlur(img_copy, (7,7), 0)\n",
    "img_gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
    "_, img_thresh = cv2.threshold(img_gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "img_final = cv2.resize(img_thresh, (28,28))\n",
    "img_final =np.reshape(img_final, (1,28,28,1))\n",
    "\n",
    "img_pred = word_dict[np.argmax(model.predict(img_final))]\n",
    "\n",
    "cv2.putText(img, \"Character Prediction: \" + img_pred, (10,410), cv2.FONT_HERSHEY_SIMPLEX, fontScale= 1, thickness=2, color = (0,0,255))\n",
    "cv2.imshow('Character Recognition', img)\n",
    "\n",
    "while (1):\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
